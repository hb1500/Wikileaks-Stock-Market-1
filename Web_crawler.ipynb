{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installed Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake-useragent\n",
      "  Downloading fake-useragent-0.1.10.tar.gz\n",
      "Building wheels for collected packages: fake-useragent\n",
      "  Running setup.py bdist_wheel for fake-useragent ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jieyuwang/Library/Caches/pip/wheels/07/04/1d/bbd8ba7d692add504b44552504b7df239bddf56aa3387cee2b\n",
      "Successfully built fake-useragent\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-0.1.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup\n",
      "  Downloading BeautifulSoup-3.2.1.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/pd/lj_g7cpj6nq6jqn0_47v2fxc0000gn/T/pip-build-6a69sbyc/BeautifulSoup/setup.py\", line 22\n",
      "        print \"Unit tests have failed!\"\n",
      "                                      ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print(int \"Unit tests have failed!\")?\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/pd/lj_g7cpj6nq6jqn0_47v2fxc0000gn/T/pip-build-6a69sbyc/BeautifulSoup/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Chrome WebDriver: https://sites.google.com/a/chromium.org/chromedriver/getting-started\n",
    "\n",
    "Then include the ChromeDriver location in your PATH environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def craw_one_page(url):\n",
    "    user_agent = UserAgent()\n",
    "    page = requests.get(url,headers={'user-agent':user_agent.chrome})\n",
    "    content=page.content.decode('utf-8')\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    \n",
    "    dict_result={}\n",
    "    \n",
    "    source=soup.find_all('div',attrs={'pane-header'})\n",
    "    tabs = [div.string for div in source]\n",
    "    tabs=tabs[:-2]\n",
    "    \n",
    "    \n",
    "    source=soup.find_all('div',attrs={'text-content'})\n",
    "    tabs_values = [div.string for div in source]\n",
    "    \n",
    "    for i in range(len(tabs)):\n",
    "        dict_result[tabs[i]]=tabs_values[i]\n",
    "    \n",
    "    source=soup.find_all('a',attrs={'i'})\n",
    "    keys = [div[\"title\"] for div in source]\n",
    "    \n",
    "    source=soup.find_all('div',attrs={'s_val'})\n",
    "    values = [div[\"title\"] for div in source]\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        dict_result[keys[i]]=values[i]\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get(\"https://search.wikileaks.org/plusd/?qproject[]=ps&qproject[]=cc&qproject[]=fp&qproject[]=ee&qproject[]=cg&q=China#result\", headers={'user-agent':user_agent.chrome})\n",
    "content=page.content.decode('utf-8')\n",
    "soup = BeautifulSoup(content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tagged_text=soup.find_all('a',target={'_blank'}, )\n",
    "products = [div.string for div in Tagged_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "def crawl_search_source(search_url):\n",
    "    browser=webdriver.Chrome()\n",
    "    browser.get(search_url)\n",
    "    try:\n",
    "        ele=browser.find_element_by_id(\"button_next500\")\n",
    "    except:\n",
    "        pass\n",
    "    i=0\n",
    "    while (i<500):\n",
    "        try:\n",
    "            ele.click()\n",
    "        except:\n",
    "            time.sleep(0.1)\n",
    "            i=i+1\n",
    "    return browser.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links_from_source(soup):\n",
    "    Tagged_text=soup.find_all('a',target={'_blank'}, )\n",
    "    set_url=set()\n",
    "    for x in Tagged_text:\n",
    "        url=x[\"href\"]\n",
    "        if (url[0:14]==\"/plusd/cables/\"):\n",
    "            set_url.add(x[\"href\"])\n",
    "    return set_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_obj(data):\n",
    "    with open('data_crawled.json', 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_search_result(set_url):\n",
    "    url_head= 'https://search.wikileaks.org'\n",
    "    final_result={}\n",
    "    i=0\n",
    "    print (\"total number of links: {}\".format(len(set_url)))\n",
    "    for x in set_url:\n",
    "        print (x[14:-5], \"{} out of {} jobs done\".format(i+1, len(set_url)))\n",
    "        final_result[x[14:]]=craw_one_page(url_head+x)\n",
    "        if (i/2000==0):\n",
    "            save_obj(final_result)\n",
    "        i=i+1\n",
    "    save_obj(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_url=\"https://search.wikileaks.org/plusd/?qproject[]=ps&qproject[]=cg&qproject[]=cc&qproject[]=fp&qproject[]=ee&q=&qtfrom=1972-01-01&qtto=1972-12-31#result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of links: 4\n",
      "72TEHRAN1381_a 1 out of 4 jobs done\n",
      "72TEHRAN5055_a 2 out of 4 jobs done\n",
      "72TEHRAN1164_a 3 out of 4 jobs done\n",
      "72TEHRAN4789_a 4 out of 4 jobs done\n"
     ]
    }
   ],
   "source": [
    "source=crawl_search_source(search_url)\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "set_url=find_links_from_source(soup)\n",
    "crawl_search_result(set_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=soup.find_all('p', style={\"font-size:14px\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p style=\"font-size:14px\">Total number of matching documents: 4. <a class=\"st\" href=\"#st\">Modify search</a></p>\n"
     ]
    }
   ],
   "source": [
    "for div in a:\n",
    "    print (div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
